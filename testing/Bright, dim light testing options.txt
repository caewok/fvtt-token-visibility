Options for detecting bright/dim/dark lighting/sound on target token:

litTokenBorder
--> Use dim and bright versions of litTokenBorder
--> take the dim shape of each light and intersect with the bright radius

Algorithms:
• Points: Test if lit token shape contains points Equivalent to Foundry version of w/in light border
• Pixel sampled: Use lit token shape for target
• Per pixel: Use lit token shape for target
• Geometry: Use lit token shape for target
• webGL: Use lit token shape for target

For each algorithm, can run repeatedly to determine difference between:
- unconstrained or constrained
- bright constrained
- dim constrained


Points occlusion testing
--> For 1+ points that can be seen from the viewer, test if occluded from light.

Algorithms:
• Points: For selected point, test occlusion from viewer and light
• Pixel sampled: For each sampled pixel, test occlusion from viewer and light
• Per pixel: For each pixel, test occlusion from viewer and light
• Geometry: Take the final perspective shapes, transform back to world coordinates, then test.
  --> per pixel testing: sample the pixels of the transformed shape.
  --> area testing: "face-melting" by taking view from each light,
      cutting out the obstacles of the facing target faces,
      then taking the remainder to measure area from viewpoint. Separately for dim, bright, none.
      Could even cache the cut-out target faces
• webGL: n/a; fall back on lit token shape? Or pixel sampled if fast? Could do per-pixel from the returned pixelCache, but would be slow.


Algorithm-specific testing
--> Use an exact approach specific to the algorithm
• Points: N/A. Use the Points occlusion approach, which would be "exact" for this algorithm
• Pixel sampled: N/A. Use the Points occlusion approach
• Per pixel: N/A. Use the Points occlusion approach
• Geometry: Take the final perspective shapes, transform back to world coordinates, then test.
   --> geometry testing: change view to light; run geometry test again.
• webGL: BVH or Raymarching


Algorithms:
• Points: Select points w/in 2d (or 3d) token to test
• Pixel sampled: Select points on visible token faces. No perspective mapping; stay in 3d model space.
• Per pixel: Perspective mapping, mimic webGL. Convert pixel back to world space to test lighting.
• Geometry: Perspective mapping, but use Clipper to get exact obscured geometry. Transform geometry back to world space to test lighting.
• webGL: Render target and obstacles. Test lighting within webGL by passing light uniforms and obstacle maps
  --> Can avoid passing all obstacles by passing only those within the light view
  --> Could render repeatedly, once per light. See #7 or #8

------------------
Grid shape area

Goal: large token is visible if an equivalent of 1 grid space worth of "token" is visible.

Ex: Grid square 100 x 100.
Large token that takes up 4 squares. Viewed head on
•-•-•
| | |
•-•-•
| | |
•-•-•

Total area: 200 x 200 = 40_000
Grid area: 100 x 100 = 10_000
So 1/4.

To a rough approximation, can calculate large target area by dividing the area by width * height.
Token 2 x 3 = 6.
Total area 200 x 300 = 60_000
Grid area: 10_000




------------------

1. Points
Test separately using X points on the token. Random or (possibly better) use grid based on bounds

Pro:
• Simple, fast.
• Can increase resolution by testing multiple points.
• Can choose points on the token 3d surface to improve accuracy.
• Can choose points for the facing sides to improve accuracy.
• Use the 3d token bounds or a sphere instead of the exact token shape for simplicity, speed.

Con:
• Inexact.
• Not great with tiles.

2. Per light render using Geometry
Using either the 3d token bounds or the exact token shape, use the geometric method
to draw the bright/dim/dark portions of each token face from point of view of each light.

Must convert each face to the different perspective views.
--> Convert polygons to worldview and then to distinct camera view.

e.g.
Light 1: Convert token face(s) to light view; render; convert results back.
Light 2: Convert faces to light view; render; convert results back.
Light 3: Convert faces to light view; render; convert results back.
--> Now combine all 3 for each relevant face.
--> Then render from viewer, using the results of each face.

Pro:
• Geometry method is fast
• Can ignore unseen faces
• Can store face once created until token moves or scene objects change
• Exact
• No webGL

Con:
• Tokens move a lot, so storing the lit information not as useful
• Per light x per face rendering.
• Complex conversions and polygon combinations
• Performance?

3. BVH
Put all scene objects into a bvh. Probably using triangles.
When rendering target, test for light occlusion (and viewer occlusion) using the bvh.
Likely pass through lighting information as uniforms, and limit to X nearest lights.

Need a data texture for:
1. Walls
2. Tiles
3. Regions (this one is difficult unless storing only triangles)
4. Tokens (incl custom tokens, but that could get large)

Tiles: Limited number of textures can be used. Or use a texture array or combine multiple texture to one.

Pro:
• Accuracy.
• Handles occlusion from viewer and lights using same BVH.
• Only one render required.
• Can skip lights if occluded view or a bright light found.

Con:
• Data textures (at least 4).
• BVH is complex.
• Limits on handling tile exclusion. Alternative: Convert tiles to triangles as with geometric.
• Limits for number of lights to test.
• Performance takes a hit if many lights to test. Intersection test required.
• Probably need to make everything triangles for bvh, which will increase data required to pass through.


4. Per light render using cubemap templates (lightbox)

Render lighting separately as distinct views, like in #2. But store in render textures, combine
RTs into a cubemap.

Then when rendering the target, sample from the cubemap.

Pro:
• Can store face once created until token moves or scene objects change.
• Can render unlimited number of textures.
• Uses existing rendering approach, with vertices/indices.

Con:
• Tokens move a lot, so storing the lit information not as useful
• Per light x per face rendering.
• Conversion to cubemap could be quite complex, particularly when combining render for each light.

5. Raymarching
Store information about scene object positions but use sdfs to construct.
Use intersection information to determine tile UVs and transparency.
Likely pass through lighting information as uniforms, and limit to X nearest lights.

Pro:
• Accuracy.
• SDF is relatively straightforward and the shapes can be defined explicitly w/o triangles.
• May be useful for elevated shadow lighting.
• Much less data needed compared to the triangles of #3 (bvh) or #4 (cube maps).
• Can skip lights if occluded view or a bright light found..

Con:
• Textures remain complicated.
• Performance takes a hit if many lights to test.
• Performance may suffer given number of unique objects that must be combined for a scene.
  Essentially rendering the entire scene at every march step, for every fragment.

6. Combine Raymarching with #4 (cube templates)
Raymarch, but from each light perspective separately. Sample from the cubemap instead of
testing for X light intersections.

7. webGL with multiple renders using vertices/indices
Render target normally with obstacles using vertices/indices. Render to RT.
For each light,
  B. render each obstacle from view of light and compare to the RT. Mark RT accordingly.

Would require knowing the translation to/from world space for each pixel in the original render.
Which we do, b/c it is the modelview matrix.
Ideally, we would use B. Would need to:
 - Render initial target and obstacles from viewer perspective.
 - Render obstacles and target from light perspective.
   - At each fragment, determine the approximate pixel location of the original render.
   - Pull the RT at that location and update
   - So it is rerendering the RT
Probably doesn't work using webGL as would need to render a quad to rerender the RT.
But already rendering a different viewpoint (from the light).

Could we render each target face and transform it so it fits squarely within an RT?
Then we are back at something like a cubemap, pulling from it when rendering the target from the viewer viewpoint
Trick is we need to render using some sort of perspective, but then undo that perspective.


8. webGL with multiple renders and data textures
Render target normally with obstacles using vertices/indices. Render to RT.
For each light:
  - re-render the target shape.
  - pass all obstacles for the given light as data textures; for each fragment, test intersection
  - pull from the original target render and modify with the intersection test.

Works better than doing all at once where we don't really need to test that many obstacles.
Avoids importing the world scene but does require a lot of data texture modification and uploading
Probably faster to load everything all at once and use BVH


Current benchmarks:

This is the End (487 walls, no tiles)
points: 0.43 ms
geometric: 1.59 ms
perPixel: 14.09 ms
webGL2: 9.9 ms

Test Edges (w/ tiles)
points: 1.6 ms
geometric: 3.79 ms
perPixel: 61.96 ms
webGL2: 35.61 ms

If we had 5 lights hitting any given token, then could need:
5 x 3 faces = 15 renders

This is the End
points: ??
geometric per face: 1.59 + 1.59*15 = 9.54 ms
perPixel: 14.09 ms
webGL2 w/ BVH: > 9.9 ms
webGL2 w/ cubemap: 9.9ms + 9.9 * 15 = 158.4 ms
webGL2 w/ Raymarching: ??

Test Edges (w/ tiles)
 points: ??
geometric per face: 3.79 + 3.79*15 = 60.84 ms
perPixel: 14.09 ms
webGL2 w/ BVH: > 35.61 ms
webGL2 w/ cubemap: 35.61ms + 35.61 * 15 = 213.66 ms
webGL2 w/ Raymarching: ??

--> Probably not worth pursuing cubemaps given that tokens move a lot.
A given token moving would require:
- Redraw cubemaps: 213.66 ms
- Test each viewing token: 35.61ms * number of Tokens
--> If 10 tokens, at 569 ms

Versus BVH:
- Test each token. So can take up to 56.9 ms per token before cubemaps is better.
  Nearly double the time.



